# 最適化アルゴリズム

## 最急降下法と確率的勾配法

* [最急降下法(Steepest descent method)(Wikipedia)](https://ja.wikipedia.org/wiki/%E6%9C%80%E6%80%A5%E9%99%8D%E4%B8%8B%E6%B3%95)
* [確率的勾配法(Stochastic Gradient Descent, SGD)(Wikipedia)](https://ja.wikipedia.org/wiki/%E7%A2%BA%E7%8E%87%E7%9A%84%E5%8B%BE%E9%85%8D%E9%99%8D%E4%B8%8B%E6%B3%95)

最急降下法と確率的勾配法の違い
* 最急降下法
    * 学習データを全て使って勾配を求め、パラメータ更新を行う。
* 確率的勾配法
    * 学習データの一部をランダムに選択し、選択したデータを使って勾配を求める。選択した一部のデータをミニバッチと呼ぶ。
    epoch毎に学習データをランダムに並び替えて一定数ずつ取り出す手法がよく使われる。
    * 学習データの一部を使うので、最急降下法に比べiteration毎の計算量が少ない。
    * 最急降下法は局所最適解に陥りやすいが、確率的勾配法はより適切な解を発見しやすい。

## モーメントつきSGD

パラメータの変更量に前回のパラメータの変更量の一定割合を加える手法をモーメント付きSGD(Momentum SGD)と呼ぶ。モーメントつきにすることで収束を早めることができる。

## 学習率の決め方

学習率を決めるときは、最初に大きめの値を試み、損失関数が発散するようなら小さくしていく、という方法をとると良い。

## 学習率のスケジューリング

SGDでは学習率を大きくすると収束が早くなるが、損失関数の変動が不安定になる。この問題を解決するために、損失関数がある程度収束したら学習率を小さくする手法がある。一定のepochまたはiteration毎に学習率を0.1倍したり、全体epoch数の50%と75%で学習率を0.1倍する手法などがある。

## SGDの改良アルゴリズム

[An overview of gradient descent optimization algorithms](http://sebastianruder.com/optimizing-gradient-descent/) [日本語訳](http://postd.cc/optimizing-gradient-descent/)

## どのアルゴリズムを使うのがよいか？

アルゴリズムがいくつもあって迷うが、最初はAdamを使ってみるのが良いと思う。

SGDの改良アルゴリズムがいくつか提案されているが、(モーメントつき)SGDも廃れているわけではなくよく使われている。
